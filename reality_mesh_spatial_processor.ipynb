{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒ Reality Mesh AI - Spatial Understanding Processor\n",
    "\n",
    "This notebook processes data captured from the Reality Mesh AI app and builds spatial understanding models.\n",
    "\n",
    "## Features:\n",
    "- ðŸ“Š Load and parse Reality Mesh sensor + point cloud data\n",
    "- ðŸ§­ Sensor fusion (GPS, gyro, accel, magnetometer)\n",
    "- ðŸ—ºï¸ 3D point cloud reconstruction\n",
    "- ðŸ“ Spatial mapping and localization\n",
    "- ðŸŽ¨ Interactive 3D visualization\n",
    "- ðŸ¤– Export for AI/ML training\n",
    "\n",
    "## Upload Your Data:\n",
    "1. Capture data using: https://ibreitenbach.github.io/reality-mesh-ai/\n",
    "2. Export the JSON file\n",
    "3. Upload it to this Colab session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q numpy pandas matplotlib plotly opencv-python scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "from scipy.spatial.transform import Rotation\n",
    "from sklearn.cluster import DBSCAN\n",
    "from google.colab import files\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Upload Reality Mesh Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload JSON file from Reality Mesh app\n",
    "print(\"ðŸ“ Upload your reality-mesh-sensor-data-*.json file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load the first uploaded file\n",
    "filename = list(uploaded.keys())[0]\n",
    "print(f\"\\nâœ… Loaded: {filename}\")\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"\\nðŸ“Š Data Summary:\")\n",
    "print(f\"  Frames captured: {data['metadata']['frameCount']}\")\n",
    "print(f\"  Point cloud size: {data['metadata']['pointCount']}\")\n",
    "print(f\"  Sensor samples: {data['metadata']['sensorSamples']}\")\n",
    "print(f\"  Duration: {data['metadata']['duration']:.2f} seconds\")\n",
    "print(f\"  Device: {data['metadata']['device'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Parse Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sensor data into DataFrames\n",
    "sensor_records = []\n",
    "\n",
    "for sample in data['sensors']:\n",
    "    record = {\n",
    "        'frame': sample['frame'],\n",
    "        'timestamp': sample['timestamp'],\n",
    "        # Orientation\n",
    "        'orient_alpha': float(sample['orientation']['alpha']),\n",
    "        'orient_beta': float(sample['orientation']['beta']),\n",
    "        'orient_gamma': float(sample['orientation']['gamma']),\n",
    "        # Gyroscope\n",
    "        'gyro_x': float(sample['gyroscope']['x']),\n",
    "        'gyro_y': float(sample['gyroscope']['y']),\n",
    "        'gyro_z': float(sample['gyroscope']['z']),\n",
    "        # Accelerometer\n",
    "        'accel_x': float(sample['accelerometer']['x']),\n",
    "        'accel_y': float(sample['accelerometer']['y']),\n",
    "        'accel_z': float(sample['accelerometer']['z']),\n",
    "        # GPS\n",
    "        'gps_lat': float(sample['gps']['lat']),\n",
    "        'gps_lon': float(sample['gps']['lon']),\n",
    "        'gps_alt': float(sample['gps']['alt']),\n",
    "        'gps_accuracy': float(sample['gps']['accuracy']),\n",
    "        # Light\n",
    "        'light': float(sample['light']) if sample['light'] != 'N/A' else 0\n",
    "    }\n",
    "    sensor_records.append(record)\n",
    "\n",
    "df_sensors = pd.DataFrame(sensor_records)\n",
    "df_sensors['time'] = (df_sensors['timestamp'] - df_sensors['timestamp'].min()) / 1000  # Convert to seconds\n",
    "\n",
    "print(\"\\nðŸ“Š Sensor DataFrame:\")\n",
    "print(df_sensors.head())\n",
    "print(f\"\\nShape: {df_sensors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Parse Point Cloud Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract point cloud\n",
    "points = []\n",
    "colors = []\n",
    "\n",
    "for point in data['pointCloud']:\n",
    "    points.append([point['x'], point['y'], point['z']])\n",
    "    colors.append([point['r'], point['g'], point['b']])\n",
    "\n",
    "points = np.array(points)\n",
    "colors = np.array(colors)\n",
    "\n",
    "print(f\"\\nâ˜ï¸ Point Cloud:\")\n",
    "print(f\"  Total points: {len(points):,}\")\n",
    "print(f\"  X range: [{points[:,0].min():.2f}, {points[:,0].max():.2f}]\")\n",
    "print(f\"  Y range: [{points[:,1].min():.2f}, {points[:,1].max():.2f}]\")\n",
    "print(f\"  Z range: [{points[:,2].min():.2f}, {points[:,2].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Sensor Fusion & Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_device_poses(df):\n",
    "    \"\"\"\n",
    "    Compute device pose (position + orientation) from sensor data\n",
    "    \"\"\"\n",
    "    poses = []\n",
    "    \n",
    "    # Initial position (from GPS)\n",
    "    base_lat = df['gps_lat'].iloc[0]\n",
    "    base_lon = df['gps_lon'].iloc[0]\n",
    "    base_alt = df['gps_alt'].iloc[0]\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        # Convert GPS to relative meters (approximate)\n",
    "        lat_diff = (row['gps_lat'] - base_lat) * 111320  # meters per degree latitude\n",
    "        lon_diff = (row['gps_lon'] - base_lon) * 111320 * np.cos(np.radians(base_lat))\n",
    "        alt_diff = row['gps_alt'] - base_alt\n",
    "        \n",
    "        # Orientation as rotation matrix\n",
    "        r = Rotation.from_euler('zyx', [\n",
    "            np.radians(row['orient_alpha']),\n",
    "            np.radians(row['orient_beta']),\n",
    "            np.radians(row['orient_gamma'])\n",
    "        ])\n",
    "        \n",
    "        poses.append({\n",
    "            'frame': row['frame'],\n",
    "            'time': row['time'],\n",
    "            'position': np.array([lon_diff, lat_diff, alt_diff]),\n",
    "            'rotation': r,\n",
    "            'rotation_matrix': r.as_matrix()\n",
    "        })\n",
    "    \n",
    "    return poses\n",
    "\n",
    "poses = compute_device_poses(df_sensors)\n",
    "print(f\"\\nðŸ“ Computed {len(poses)} device poses\")\n",
    "print(f\"  Movement range: {np.linalg.norm([p['position'] for p in poses], axis=1).max():.2f} meters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sensor readings over time\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "\n",
    "# Orientation\n",
    "axes[0,0].plot(df_sensors['time'], df_sensors['orient_alpha'], label='Alpha (Yaw)')\n",
    "axes[0,0].plot(df_sensors['time'], df_sensors['orient_beta'], label='Beta (Pitch)')\n",
    "axes[0,0].plot(df_sensors['time'], df_sensors['orient_gamma'], label='Gamma (Roll)')\n",
    "axes[0,0].set_title('Device Orientation')\n",
    "axes[0,0].set_xlabel('Time (s)')\n",
    "axes[0,0].set_ylabel('Degrees')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True)\n",
    "\n",
    "# Gyroscope\n",
    "axes[0,1].plot(df_sensors['time'], df_sensors['gyro_x'], label='X')\n",
    "axes[0,1].plot(df_sensors['time'], df_sensors['gyro_y'], label='Y')\n",
    "axes[0,1].plot(df_sensors['time'], df_sensors['gyro_z'], label='Z')\n",
    "axes[0,1].set_title('Gyroscope (Rotation Rate)')\n",
    "axes[0,1].set_xlabel('Time (s)')\n",
    "axes[0,1].set_ylabel('deg/s')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True)\n",
    "\n",
    "# Accelerometer\n",
    "axes[1,0].plot(df_sensors['time'], df_sensors['accel_x'], label='X')\n",
    "axes[1,0].plot(df_sensors['time'], df_sensors['accel_y'], label='Y')\n",
    "axes[1,0].plot(df_sensors['time'], df_sensors['accel_z'], label='Z')\n",
    "axes[1,0].set_title('Accelerometer')\n",
    "axes[1,0].set_xlabel('Time (s)')\n",
    "axes[1,0].set_ylabel('m/sÂ²')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True)\n",
    "\n",
    "# GPS trajectory\n",
    "axes[1,1].plot(df_sensors['gps_lon'], df_sensors['gps_lat'], 'b-', marker='o', markersize=2)\n",
    "axes[1,1].set_title('GPS Trajectory')\n",
    "axes[1,1].set_xlabel('Longitude')\n",
    "axes[1,1].set_ylabel('Latitude')\n",
    "axes[1,1].grid(True)\n",
    "\n",
    "# Altitude\n",
    "axes[2,0].plot(df_sensors['time'], df_sensors['gps_alt'])\n",
    "axes[2,0].set_title('Altitude')\n",
    "axes[2,0].set_xlabel('Time (s)')\n",
    "axes[2,0].set_ylabel('Meters')\n",
    "axes[2,0].grid(True)\n",
    "\n",
    "# Light sensor\n",
    "axes[2,1].plot(df_sensors['time'], df_sensors['light'])\n",
    "axes[2,1].set_title('Ambient Light')\n",
    "axes[2,1].set_xlabel('Time (s)')\n",
    "axes[2,1].set_ylabel('Lux')\n",
    "axes[2,1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Sensor data visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: 3D Point Cloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive 3D point cloud visualization with Plotly\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=points[:, 0],\n",
    "    y=points[:, 1],\n",
    "    z=points[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        color=['rgb({},{},{})'.format(int(r*255), int(g*255), int(b*255)) \n",
    "               for r, g, b in colors],\n",
    "        opacity=0.8\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Reality Mesh Point Cloud',\n",
    "    scene=dict(\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='Y',\n",
    "        zaxis_title='Z (Depth)',\n",
    "        aspectmode='data'\n",
    "    ),\n",
    "    width=900,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "print(\"\\nâœ… Point cloud rendered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Spatial Clustering & Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DBSCAN clustering to detect spatial objects\n",
    "print(\"ðŸ” Running spatial clustering...\")\n",
    "clustering = DBSCAN(eps=0.5, min_samples=10).fit(points)\n",
    "labels = clustering.labels_\n",
    "\n",
    "# Number of clusters\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = list(labels).count(-1)\n",
    "\n",
    "print(f\"\\nðŸ“¦ Spatial Clustering Results:\")\n",
    "print(f\"  Clusters found: {n_clusters}\")\n",
    "print(f\"  Noise points: {n_noise}\")\n",
    "\n",
    "# Visualize clusters\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=points[:, 0],\n",
    "    y=points[:, 1],\n",
    "    z=points[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        color=labels,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8,\n",
    "        showscale=True\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Spatial Clusters ({n_clusters} objects detected)',\n",
    "    scene=dict(\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='Y',\n",
    "        zaxis_title='Z',\n",
    "        aspectmode='data'\n",
    "    ),\n",
    "    width=900,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Build Spatial Understanding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialUnderstandingModel:\n",
    "    \"\"\"\n",
    "    Spatial understanding model that combines:\n",
    "    - Point cloud geometry\n",
    "    - Device trajectory\n",
    "    - Sensor context (light, orientation, etc.)\n",
    "    \"\"\"\n",
    "    def __init__(self, points, colors, poses, sensors):\n",
    "        self.points = points\n",
    "        self.colors = colors\n",
    "        self.poses = poses\n",
    "        self.sensors = sensors\n",
    "        \n",
    "    def compute_spatial_features(self):\n",
    "        \"\"\"Extract spatial features for AI/ML\"\"\"\n",
    "        features = {\n",
    "            # Geometry features\n",
    "            'point_cloud_size': len(self.points),\n",
    "            'spatial_extent': {\n",
    "                'x_range': (self.points[:,0].min(), self.points[:,0].max()),\n",
    "                'y_range': (self.points[:,1].min(), self.points[:,1].max()),\n",
    "                'z_range': (self.points[:,2].min(), self.points[:,2].max())\n",
    "            },\n",
    "            'centroid': self.points.mean(axis=0).tolist(),\n",
    "            \n",
    "            # Trajectory features\n",
    "            'num_poses': len(self.poses),\n",
    "            'total_distance': sum([\n",
    "                np.linalg.norm(self.poses[i]['position'] - self.poses[i-1]['position'])\n",
    "                for i in range(1, len(self.poses))\n",
    "            ]),\n",
    "            \n",
    "            # Sensor statistics\n",
    "            'avg_light': self.sensors['light'].mean(),\n",
    "            'orientation_variance': {\n",
    "                'alpha': self.sensors['orient_alpha'].std(),\n",
    "                'beta': self.sensors['orient_beta'].std(),\n",
    "                'gamma': self.sensors['orient_gamma'].std()\n",
    "            },\n",
    "            'gps_accuracy': self.sensors['gps_accuracy'].mean()\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def export_for_training(self):\n",
    "        \"\"\"Export data in format suitable for ML training\"\"\"\n",
    "        return {\n",
    "            'geometry': {\n",
    "                'points': self.points.tolist(),\n",
    "                'colors': self.colors.tolist()\n",
    "            },\n",
    "            'trajectory': [\n",
    "                {\n",
    "                    'frame': p['frame'],\n",
    "                    'position': p['position'].tolist(),\n",
    "                    'rotation_matrix': p['rotation_matrix'].tolist()\n",
    "                }\n",
    "                for p in self.poses\n",
    "            ],\n",
    "            'sensors': self.sensors.to_dict('records'),\n",
    "            'features': self.compute_spatial_features()\n",
    "        }\n",
    "\n",
    "# Create model\n",
    "model = SpatialUnderstandingModel(points, colors, poses, df_sensors)\n",
    "features = model.compute_spatial_features()\n",
    "\n",
    "print(\"\\nðŸ§  Spatial Understanding Features:\")\n",
    "print(json.dumps(features, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Device Trajectory Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract trajectory positions\n",
    "trajectory = np.array([p['position'] for p in poses])\n",
    "\n",
    "# Create 3D trajectory plot with orientation arrows\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add trajectory line\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=trajectory[:, 0],\n",
    "    y=trajectory[:, 1],\n",
    "    z=trajectory[:, 2],\n",
    "    mode='lines+markers',\n",
    "    marker=dict(size=5, color='red'),\n",
    "    line=dict(color='red', width=3),\n",
    "    name='Device Path'\n",
    "))\n",
    "\n",
    "# Add start and end points\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=[trajectory[0, 0]],\n",
    "    y=[trajectory[0, 1]],\n",
    "    z=[trajectory[0, 2]],\n",
    "    mode='markers',\n",
    "    marker=dict(size=10, color='green', symbol='diamond'),\n",
    "    name='Start'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=[trajectory[-1, 0]],\n",
    "    y=[trajectory[-1, 1]],\n",
    "    z=[trajectory[-1, 2]],\n",
    "    mode='markers',\n",
    "    marker=dict(size=10, color='blue', symbol='diamond'),\n",
    "    name='End'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Device Trajectory (GPS-based)',\n",
    "    scene=dict(\n",
    "        xaxis_title='East-West (m)',\n",
    "        yaxis_title='North-South (m)',\n",
    "        zaxis_title='Altitude (m)',\n",
    "        aspectmode='data'\n",
    "    ),\n",
    "    width=900,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nðŸ“ Total distance traveled: {features['total_distance']:.2f} meters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Export Processed Data for AI Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export complete spatial understanding dataset\n",
    "training_data = model.export_for_training()\n",
    "\n",
    "# Save to file\n",
    "output_filename = 'spatial_understanding_training_data.json'\n",
    "with open(output_filename, 'w') as f:\n",
    "    json.dump(training_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Training data exported to: {output_filename}\")\n",
    "print(f\"\\nðŸ“Š Dataset Summary:\")\n",
    "print(f\"  Point cloud: {len(training_data['geometry']['points']):,} points\")\n",
    "print(f\"  Trajectory: {len(training_data['trajectory'])} poses\")\n",
    "print(f\"  Sensor samples: {len(training_data['sensors'])}\")\n",
    "print(f\"\\nâœ… Ready for AI/ML model training!\")\n",
    "\n",
    "# Download the file\n",
    "files.download(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Generate Spatial Understanding Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“‹ SPATIAL UNDERSTANDING REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nðŸŽ¥ Capture Session:\")\n",
    "print(f\"  Duration: {data['metadata']['duration']:.2f} seconds\")\n",
    "print(f\"  Frames: {data['metadata']['frameCount']}\")\n",
    "print(f\"  Device: {data['metadata']['device'][:50]}\")\n",
    "\n",
    "print(f\"\\nâ˜ï¸ Point Cloud:\")\n",
    "print(f\"  Total points: {len(points):,}\")\n",
    "print(f\"  Spatial extent:\")\n",
    "print(f\"    X: {features['spatial_extent']['x_range'][0]:.2f} to {features['spatial_extent']['x_range'][1]:.2f}\")\n",
    "print(f\"    Y: {features['spatial_extent']['y_range'][0]:.2f} to {features['spatial_extent']['y_range'][1]:.2f}\")\n",
    "print(f\"    Z: {features['spatial_extent']['z_range'][0]:.2f} to {features['spatial_extent']['z_range'][1]:.2f}\")\n",
    "print(f\"  Centroid: ({features['centroid'][0]:.2f}, {features['centroid'][1]:.2f}, {features['centroid'][2]:.2f})\")\n",
    "\n",
    "print(f\"\\nðŸ“ Device Trajectory:\")\n",
    "print(f\"  Poses captured: {features['num_poses']}\")\n",
    "print(f\"  Total distance: {features['total_distance']:.2f} meters\")\n",
    "print(f\"  GPS accuracy: {features['gps_accuracy']:.1f} meters\")\n",
    "\n",
    "print(f\"\\nðŸ§­ Orientation Variance:\")\n",
    "print(f\"  Alpha (Yaw): {features['orientation_variance']['alpha']:.2f}Â°\")\n",
    "print(f\"  Beta (Pitch): {features['orientation_variance']['beta']:.2f}Â°\")\n",
    "print(f\"  Gamma (Roll): {features['orientation_variance']['gamma']:.2f}Â°\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Environment:\")\n",
    "print(f\"  Avg ambient light: {features['avg_light']:.0f} lux\")\n",
    "\n",
    "if n_clusters > 0:\n",
    "    print(f\"\\nðŸ“¦ Spatial Objects:\")\n",
    "    print(f\"  Detected clusters: {n_clusters}\")\n",
    "    print(f\"  Noise points: {n_noise:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… SPATIAL UNDERSTANDING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Use exported JSON for ML model training\")\n",
    "print(\"  2. Integrate with SLAM algorithms\")\n",
    "print(\"  3. Build 3D reconstruction pipeline\")\n",
    "print(\"  4. Train spatial reasoning models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
